{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100c1437-5143-485d-afbe-c5b3f7f99497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8c1c20-48d3-4ead-8415-d4c7f5a1a57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"iPhone Store\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a4025b-6e4e-4354-8f05-b78f5b603f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_file_path = \"/FileStore/FileStore/sales_data-9.txt\"\n",
    "product_file_path = \"/FileStore/FileStore/product_data-6.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aab914b-2c59-4525-a0b8-26155212b3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sales Collector API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081d2552-c88d-4665-8c60-3cd377ba889e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n| sale_date|\n+----------+\n|2019-05-13|\n|2019-06-02|\n|2019-01-21|\n|2019-02-17|\n+----------+\n\n+---------+----------+--------+----------+--------+-----+\n|seller_id|product_id|buyer_id| sale_date|quantity|price|\n+---------+----------+--------+----------+--------+-----+\n|        1|         1|       1|2019-01-21|       2| 2000|\n|        1|         2|       2|2019-02-17|       1|  800|\n|        2|         1|       3|2019-06-02|       1|  800|\n|        3|         3|       3|2019-05-13|       2| 2800|\n+---------+----------+--------+----------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "def sales_data_collector_api(spark, sales_file_path):\n",
    "    # Step 1: Load Sales Data from the Text File with Proper Header Handling\n",
    "    sales_df_raw = spark.read.option(\"delimiter\", \"|\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(sales_file_path)\n",
    "\n",
    "    # Check distinct sale_date values to confirm the format\n",
    "    sales_df_raw.select(\"sale_date\").distinct().show()\n",
    "\n",
    "    # Step 2: Filter Out Any Invalid Header Row (if necessary)\n",
    "    sales_df = sales_df_raw.filter(\"seller_id IS NOT NULL AND sale_date IS NOT NULL\")\n",
    "\n",
    "    # Step 3: Cast Columns to Correct Data Types\n",
    "    sales_df = sales_df.select(\n",
    "        col(\"seller_id\").cast(\"int\"),\n",
    "        col(\"product_id\").cast(\"int\"),\n",
    "        col(\"buyer_id\").cast(\"int\"),\n",
    "        to_date(col(\"sale_date\"), \"yyyy-MM-dd\").alias(\"sale_date\"),  # Correctly cast sale_date\n",
    "        col(\"quantity\").cast(\"int\"),\n",
    "        col(\"price\").cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return sales_df\n",
    "\n",
    "\n",
    "# Call the function to collect sales data\n",
    "sales_df = sales_data_collector_api(spark, sales_file_path)\n",
    "\n",
    "# Save sales DataFrame as a Hive table in Parquet format\n",
    "sales_df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"sales_table_parquet\")\n",
    "\n",
    "# Query the Hive Table to Confirm Data is Written\n",
    "result_df = spark.sql(\"SELECT * FROM sales_table_parquet\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a812bc-faed-485c-818d-7aeb38f91c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**'''The sales_data_collector_api function loads sales data from a text file, specifying the delimiter as | and that the first row contains headers. It then filters out rows with NULL values in critical fields like seller_id and sale_date to ensure data integrity. The function proceeds to cast the columns to their appropriate data types, converting sale_date to a date format and casting the numeric fields (seller_id, product_id, buyer_id, quantity, and price) to integers. After the data is cleaned and transformed, the resulting DataFrame is saved to a Hive table named sales_table_parquet in Parquet format, overwriting any existing table. Finally, a query is run to confirm the data has been written correctly, displaying the content of the table. This process ensures that sales data is properly structured and stored in a Hive table for further analysis.\n",
    "'''**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb34cb64-3786-4e02-aa0b-e53f88c7fde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Product Collector API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636deb93-6b47-4acb-b526-10b9b8d33293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+\n|product_id|product_name|unit_price|\n+----------+------------+----------+\n|         1|          S8|      1000|\n|         2|          G4|       800|\n|         3|      iPhone|      1400|\n+----------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "def product_data_collector_api(spark, product_file_path):\n",
    "    # Step 1: Load Product Data from the CSV File and assign column names\n",
    "    product_df = spark.read.option(\"delimiter\", \"|\").csv(product_file_path, header=True) \n",
    "    \n",
    "    # Step 2: Cast Columns to Correct Data Types\n",
    "    product_df = product_df.select(\n",
    "        F.col(\"product_id\").cast(\"int\"),  \n",
    "        F.col(\"product_name\").cast(\"string\"),  \n",
    "        F.col(\"unit_price\").cast(\"int\") \n",
    "    )\n",
    "\n",
    "    # Step 3: Write the Data to the Hive Table in Parquet format\n",
    "    product_df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"product_table_parquet\")\n",
    "\n",
    "\n",
    "# Call the function to collect product data\n",
    "product_data_collector_api(spark, product_file_path)\n",
    "\n",
    "# Step 4: Query the Hive Table to Confirm Data is Written\n",
    "result_df = spark.sql(\"SELECT * FROM product_table_parquet\")\n",
    "result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59db2357-36cb-458c-9f73-97a53c44ccf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**'''\n",
    "The product_data_collector_api function loads product data from a CSV file, specifying `|` as the delimiter and treating the first row as headers. It then casts the columns to the appropriate data types: product_id is converted to an integer, product_name is cast to a string, and unit_price is also cast to an integer. After processing and ensuring the data is properly formatted, the resulting DataFrame is written to a Hive table named product_table_parquet in Parquet format, with the option to overwrite any existing data in the table. Finally, the function queries the Hive table to confirm that the data has been successfully written, displaying the table's content for verification. This process ensures that the product data is stored in a structured, queryable format in Hive for further analysis.\n",
    "'''**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b80bf000-dac9-44cd-a502-2e53a3945815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data Preparation API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c859354a-778b-4e68-848f-a26667d688e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|buyer_id|\n+--------+\n|       1|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "def get_buyers_s8_not_iphone(spark, product_table_parquet, sales_table_parquet, target_table):\n",
    "    # Step 1: Load Data from Product and Sales Hive Tables\n",
    "    product_df = spark.sql(f\"SELECT * FROM {product_table_parquet}\")\n",
    "    sales_df = spark.sql(f\"SELECT * FROM {sales_table_parquet}\")\n",
    "\n",
    "    # Step 2: Get Product IDs for 'S8' and 'iPhone'\n",
    "    s8_product_row = product_df.filter(\"product_name = 'S8'\").select(\"product_id\").first()\n",
    "    iphone_product_row = product_df.filter(\"product_name = 'iPhone'\").select(\"product_id\").first()\n",
    "    \n",
    "    # Check if product IDs are found\n",
    "    if not s8_product_row or not iphone_product_row:\n",
    "        raise ValueError(\"One or both of the products 'S8' or 'iPhone' are not found in the product table.\")\n",
    "    \n",
    "    s8_product_id = s8_product_row['product_id']\n",
    "    iphone_product_id = iphone_product_row['product_id']\n",
    "    \n",
    "    # Step 3: Get Buyers for 'S8' (distinct buyer_id)\n",
    "    s8_buyers = sales_df.filter(f\"product_id = {s8_product_id}\").select(\"buyer_id\").distinct()\n",
    "    \n",
    "    # Get Buyers for 'iPhone' (distinct buyer_id)\n",
    "    iphone_buyers = sales_df.filter(f\"product_id = {iphone_product_id}\").select(\"buyer_id\").distinct()\n",
    "    \n",
    "    # Step 4: Find Buyers who bought 'S8' but not 'iPhone' (left anti join)\n",
    "    buyers_s8_not_iphone = s8_buyers.join(iphone_buyers, \"buyer_id\", \"left_anti\")\n",
    "    \n",
    "    \n",
    "    # Step 6: Write the Result to the Target Hive Table\n",
    "    buyers_s8_not_iphone.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "\n",
    "# Call the function and store the result\n",
    "buyers_s8_not_iphone_result = get_buyers_s8_not_iphone(spark, 'product_table_parquet', 'sales_table_parquet', 'buyers_s8_not_iphone_table') \n",
    "\n",
    "# Step 7: Query the Target Hive Table to Confirm Data is Written\n",
    "result_df = spark.sql(f\"SELECT * FROM {target_table}\")\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3ba215-752d-4b93-bd4b-bab4f12973d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**'''\n",
    "The get_buyers_s8_not_iphone function loads product and sales data from Hive tables, filters the products to retrieve the IDs for 'S8' and 'iPhone', and checks if both IDs are available. It then retrieves the distinct buyer IDs for each product from the sales data. By performing a left anti join, the function identifies buyers who bought 'S8' but not 'iPhone'. The result is then written to a target Hive table in Parquet format, overwriting any existing data. Finally, the function queries the target table to confirm that the data has been successfully written and displays the result. This process is useful for analyzing customer purchasing patterns and segmenting buyers based on specific product preferences.\n",
    "'''**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc65edc7-0509-4b44-937d-cd678b321549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|buyers_s8_not_iph...|      false|\n| default|product_table_par...|      false|\n| default| sales_table_parquet|      false|\n+--------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Iphone-Sales-Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
